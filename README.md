# Ollama Arena â€” Local Multi-Model AI Comparison Platform

A privacy-first, local-only web application for side-by-side evaluation of multiple AI models via Ollama, with persistent conversation state and exportable results.

---

## ğŸ¯ Problem Statement

### Privacy and Compliance Constraints in AI Evaluation

Organizations and individuals in regulated industries (healthcare, finance, legal, government) face significant barriers when evaluating AI models:

- **Data Sensitivity**: Sending proprietary or sensitive data to cloud-based comparison platforms (ChatGPT Arena, LMSys) violates privacy policies and compliance requirements (GDPR, HIPAA, SOX)
- **API Key Risk**: Cloud tools require API keys, creating security exposure and audit complexity
- **No Control Over Data**: Once data leaves local systems, there's no guarantee of deletion, non-training use, or compliance with data residency laws
- **Vendor Lock-in**: Cloud platforms control access, pricing, and model availability

### Limitations of Cloud-Based Comparison Tools

Existing solutions like ChatGPT Arena and LMSys are excellent for public benchmarking, but fail for:

- **Private datasets**: Cannot evaluate models on proprietary or confidential information
- **Offline environments**: Require internet connectivity and external dependencies
- **Reproducibility**: No control over model versions, parameters, or state persistence
- **Cost**: API usage fees accumulate quickly during evaluation campaigns
- **Auditability**: No local logs or exportable artifacts for compliance reporting

**Ollama Arena solves this** by bringing multi-model evaluation entirely to your local machine, with zero external API calls and complete data sovereignty.

---

## ğŸ—ï¸ System Overview

### Local-First Architecture

Ollama Arena is a **100% local, zero-cloud** Flask web application that orchestrates multiple AI models through Ollama:

- **No API Keys Required**: All models run locally via Ollama's inference engine
- **No Internet Dependency**: Works completely offline (after initial model downloads)
- **Full Data Control**: Conversations never leave your machine
- **Browser-Based UI**: Modern, responsive interface accessible at `http://127.0.0.1:7860`

### Multi-Model Orchestration via Ollama

- **Arena Mode**: Send identical prompts to 2-6 models simultaneously for blind comparison
- **Single Model Mode**: Interactive chat with one model at a time
- **Dynamic Model Switching**: Start in arena mode, continue conversations with individual models
- **Real-Time Streaming**: See responses as they're generated, character by character

### Persistent State and Exportable Artifacts

- **Conversation Export**: Download full chat histories as JSON with timestamps and metadata
- **Session Persistence**: Conversations survive browser refreshes (in-memory state)
- **Model Metadata Tracking**: Records model names, response times, and token counts
- **Audit Trail**: Complete logs available in `logger.py` output for compliance verification

---

## ğŸ“Š Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     BROWSER (localhost:7860)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Web UI (templates/index.html + static/app.js)          â”‚   â”‚
â”‚  â”‚  â€¢ Arena mode (multi-model) vs Single mode              â”‚   â”‚
â”‚  â”‚  â€¢ Real-time streaming display                          â”‚   â”‚
â”‚  â”‚  â€¢ Copy, export, regenerate controls                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ HTTP/SSE (local only)
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             FLASK APP (web_chat.py + app/)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Routes & API (app/routes.py, app/api_routes.py)        â”‚  â”‚
â”‚  â”‚  â€¢ /chat (arena mode) - broadcasts to N models          â”‚  â”‚
â”‚  â”‚  â€¢ /single_chat - single model streaming                â”‚  â”‚
â”‚  â”‚  â€¢ /export - JSON conversation download                 â”‚  â”‚
â”‚  â”‚  â€¢ /models - list available Ollama models               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                       â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ollama Service (app/ollama_service.py)                 â”‚  â”‚
â”‚  â”‚  â€¢ Model validation & health checks                     â”‚  â”‚
â”‚  â”‚  â€¢ Streaming response handlers                          â”‚  â”‚
â”‚  â”‚  â€¢ Error recovery & retry logic                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ localhost:11434 (Ollama API)
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OLLAMA (Local Process)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Model Inference Engine                                  â”‚  â”‚
â”‚  â”‚  â€¢ llama3.2, qwen2.5, gemma2, etc.                       â”‚  â”‚
â”‚  â”‚  â€¢ GPU acceleration (if available)                       â”‚  â”‚
â”‚  â”‚  â€¢ Model parameter control (temp, top_p, etc.)           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ Local filesystem only
                          â–¼
               [Models stored in ~/.ollama/models/]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    ğŸš« NO CLOUD BOUNDARY ğŸš«
         ALL PROCESSING HAPPENS ON YOUR LOCAL MACHINE
         NO DATA TRANSMITTED TO EXTERNAL SERVICES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Key Data Flows:**
1. User sends prompt via browser â†’ Flask backend receives it locally
2. Flask routes to arena (multi-model) or single-model handler
3. Ollama service streams responses from local Ollama instance
4. Results streamed back to browser in real-time via Server-Sent Events
5. Export function serializes conversation to JSON (downloaded to user's machine)

---

## ğŸ§  Key Design Decisions

### Why Local-First?

**Privacy is non-negotiable** for many use cases:
- Evaluating models on confidential business data (contracts, financials, customer records)
- Testing with PII (personal identifiable information) in regulated sectors
- Research with sensitive datasets (medical records, legal documents)
- Competitive analysis using proprietary information

**Cost and control**:
- No per-token API fees (models run on your hardware)
- No rate limits or quotas
- Complete control over model versions and parameters
- Works in air-gapped or restricted network environments

### Why Side-by-Side Comparison?

Human evaluation of AI quality is **highly contextual and subjective**. Direct comparison reveals:

- **Stylistic differences**: Formal vs. conversational tone, verbosity, structure
- **Accuracy gaps**: One model hallucinates, another cites correctly
- **Reasoning quality**: Which model shows better chain-of-thought?
- **Latency vs. quality trade-offs**: Faster models may sacrifice depth

**Blind testing** (arena mode) removes bias:
- Models are anonymized as "Model A", "Model B", etc.
- Users evaluate responses on merit, not brand reputation
- Forces objective assessment of output quality

### Why Human-in-the-Loop Evaluation?

Automated metrics (perplexity, BLEU, F1) **fail to capture real-world usefulness**:

- **Context matters**: A "wrong" answer may be more helpful than a "correct" but pedantic one
- **Tone and empathy**: Critical for customer service, healthcare, education use cases
- **Domain expertise**: Only humans can judge accuracy in specialized fields (law, medicine, engineering)
- **Safety and ethics**: Automated tools miss subtle biases, offensive content, or dangerous advice

**Ollama Arena enables rapid human evaluation**:
- See 2-6 model responses instantly
- Compare side-by-side in real-time
- Export conversations for team review or compliance audits

### Why Persistent State Matters

**Reproducibility and accountability**:
- **Audit trails**: Export conversations with timestamps for compliance reporting
- **Iterative evaluation**: Refine prompts and re-test without losing history
- **Team collaboration**: Share exported JSON for peer review
- **Long-term tracking**: Monitor model performance over time as versions change

**Technical robustness**:
- In-memory session state survives browser refreshes
- Graceful error handling prevents conversation data loss
- Structured JSON exports enable integration with analysis tools

---

## ğŸ¤– AI-Assisted QA Workflow

This project was developed using **AI-assisted coding and testing** (GitHub Copilot, ChatGPT), demonstrating the powerâ€”and limitationsâ€”of AI-driven development.

### How the Testing Agent Was Used

1. **Test Case Generation**: AI generated comprehensive test scenarios for arena mode, single mode, error handling, and edge cases
2. **Bug Discovery Automation**: AI systematically tested all UI features (copy, export, regenerate, model switching) and logged failures in [BUG_FIXES.md](BUG_FIXES.md)
3. **Fix Implementation**: AI proposed code patches for 6 critical bugs, which were reviewed and applied
4. **Regression Testing**: After fixes, AI re-validated all workflows to ensure no new breaks

### What It Caught

âœ… **6 Critical Bugs Fixed** (documented in [BUG_FIXES.md](BUG_FIXES.md)):
- Missing "Copy Response" button in single-model mode
- "Continue with One Model" feature not working in arena mode
- Regenerate button triggering errors on edge cases
- Export function missing metadata fields
- Model switching race conditions
- UI state inconsistencies after rapid interactions

âœ… **Code Quality Issues**:
- Inconsistent error handling in streaming responses
- Missing input validation for model selection
- Unhandled edge cases in conversation history management

### What It Didn't Catch

âŒ **User Experience Nuances**:
- Confusing button labels (AI didn't recognize UX friction)
- Suboptimal response rendering for long outputs (required manual CSS tuning)
- Accessibility issues (keyboard navigation, screen reader support)

âŒ **Performance Edge Cases**:
- Memory leaks with 50+ message conversations (found during manual stress testing)
- Streaming lag with slow models (AI couldn't simulate real latency)

âŒ **Domain-Specific Validation**:
- Model response quality assessment (AI can't judge "good" vs. "bad" answers)
- Privacy/security review (e.g., ensuring no telemetry or logging of sensitive data)

### Why Human Oversight Remains Essential

**AI is a powerful co-pilot, not an autopilot**:
- AI-generated tests are narrow and literal (miss creative edge cases)
- AI cannot evaluate subjective quality (UX, tone, usefulness)
- AI lacks contextual judgment (business requirements, compliance needs)
- **Final accountability rests with humans** (code reviews, security audits, production deployment decisions)

**Hybrid workflow works best**:
1. Use AI for initial scaffolding, boilerplate, and systematic testing
2. Apply human judgment for architecture, UX, security, and domain correctness
3. Iterate: AI proposes, human refines, AI validates, human approves

---

## ğŸ¯ Use Cases

### 1. Research
- **Academic ML Research**: Compare model architectures on standardized benchmarks without cloud API costs
- **Prompt Engineering**: Rapidly iterate on prompt designs and see how different models respond
- **Model Selection**: Evaluate which open-source model (Llama, Qwen, Gemma) best fits your task

### 2. Regulated Environments
- **Healthcare**: Test AI assistants on synthetic patient data locally (HIPAA compliance)
- **Finance**: Evaluate models on financial reports without violating SOX/GDPR
- **Legal**: Compare legal reasoning models on case files (attorney-client privilege)
- **Government**: Air-gapped evaluation in secure environments

### 3. Enterprise Pre-Deployment Evaluation
- **Model Vetting**: Test multiple models on real-world tasks before committing to vendor contracts
- **Cost-Benefit Analysis**: Compare cloud API models (via local proxies) vs. self-hosted options
- **Team Alignment**: Export conversation samples for stakeholder review before production deployment
- **Risk Assessment**: Identify biases, hallucinations, or safety issues in candidate models

---

## ğŸš€ How to Run

### Prerequisites
- **Python 3.8+**
- **Ollama CLI** installed ([ollama.ai](https://ollama.ai))
- At least one model pulled (e.g., `ollama pull llama3.2`)

### Installation

1. **Create & activate a virtual environment:**

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1  # Windows PowerShell
# or: source .venv/bin/activate (Linux/Mac)
```

2. **Install dependencies:**

```powershell
pip install -r requirements.txt
```

3. **Pull Ollama models** (if not already done):

```bash
ollama pull llama3.2
ollama pull qwen2.5:latest
ollama pull gemma2:9b
```

### Running the Application

**Option 1: Web UI (Recommended)**

```powershell
python .\web_chat.py
```

Open http://127.0.0.1:7860 in your browser.

**Option 2: Terminal Chat**

```powershell
python .\Chatbot.py
```

### What's Included

- [web_chat.py](web_chat.py) â€” Flask app entry point
- [app/](app/) â€” Backend modules (routes, Ollama service, error handlers)
- [templates/index.html](templates/index.html) â€” Web UI template
- [static/app.js](static/app.js) â€” Frontend logic (arena mode, streaming, export)
- [static/styles.css](static/styles.css) â€” Bootstrap-based responsive design
- [Chatbot.py](Chatbot.py) â€” Terminal chatbot (single model)
- [config.py](config.py) â€” Application configuration
- [logger.py](logger.py) â€” Structured logging for debugging and audits
- [requirements.txt](requirements.txt) â€” Python dependencies

---

## ğŸ“š How to Cite

If you use Ollama Arena in your research, please cite:

```bibtex
@software{ollama_arena_2026,
  author       = {Lagad, Shubham},
  title        = {Ollama Arena: Local-First Multi-Model AI Comparison Platform},
  year         = {2026},
  month        = {January},
  url          = {https://github.com/yourusername/ollama-arena},
  note         = {Privacy-first side-by-side evaluation of local AI models via Ollama},
  license      = {MIT}
}
```

**Key Features to Highlight in Citations:**
- Local-first architecture (zero cloud dependencies)
- Multi-model orchestration for blind comparison
- Human-in-the-loop evaluation workflows
- Compliance-friendly (GDPR, HIPAA, SOX)

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

**Copyright Â© 2026 Shubham Lagad**

### Third-Party Software Notice

This software uses [Ollama](https://ollama.ai) (Â© Ollama, Inc.), which is a separate product with its own license terms. Ollama is not included in this MIT License. Please refer to [Ollama's license](https://github.com/ollama/ollama) for details.

---

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for:

- **Code of Conduct**: Respectful, inclusive collaboration
- **Development Setup**: How to fork, clone, and set up dev environment
- **Coding Standards**: Python style guide, linting rules, test requirements
- **Pull Request Process**: Branch naming, commit messages, review workflow
- **Issue Guidelines**: Bug reports, feature requests, documentation improvements

**Quick Start for Contributors:**

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make changes and test locally
4. Run linter: `pip install -r requirements-dev.txt && flake8`
5. Submit PR with clear description of changes

**Priority Areas for Contribution:**
- ğŸ”§ Accessibility improvements (ARIA labels, keyboard navigation)
- ğŸ¨ UI/UX enhancements (dark mode, mobile responsiveness)
- ğŸ“Š Export formats (CSV, Markdown, PDF)
- ğŸ§ª Automated testing (pytest suite expansion)
- ğŸ“– Documentation (tutorials, video guides, translations)

---

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/ollama-arena/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/ollama-arena/discussions)
- **Bug Reports**: See [BUG_FIXES.md](BUG_FIXES.md) for known issues and fixes
- **Changelog**: See [CHANGELOG.md](CHANGELOG.md) for version history

---

**Made with â¤ï¸ for privacy-conscious AI practitioners**
